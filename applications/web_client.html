<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>智能對話助手 (支持雙模式音頻流)</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        /* 基本样式 */
        body {
            font-family: 'Inter', sans-serif;
            margin: 0;
            background-color: #f3f4f6;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            color: #374151;
            padding: 1rem;
        }
        .container {
            background-color: white;
            padding: 2rem;
            border-radius: 0.75rem;
            box-shadow: 0 10px 15px -3px rgba(0,0,0,0.1), 0 4px 6px -2px rgba(0,0,0,0.05);
            width: 100%;
            max-width: 600px;
            display: flex;
            flex-direction: column;
            height: 95vh;
        }
        /* 按钮样式 */
        .btn {
            padding: 0.625rem 1.25rem;
            border-radius: 0.375rem;
            font-weight: 500;
            transition: background-color 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
            display: inline-flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
        }
        .btn:focus {
            outline: 2px solid transparent;
            outline-offset: 2px;
            box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.5);
        }
        .btn-primary {
            background-color: #3b82f6;
            color: white;
        }
        .btn-primary:hover {
            background-color: #2563eb;
        }
        .btn-primary:disabled {
            background-color: #9ca3af;
            cursor: not-allowed;
        }
        .btn-secondary {
            background-color: #e5e7eb;
            color: #4b5563;
        }
        .btn-secondary:hover {
            background-color: #d1d5db;
        }
        .btn-danger {
            background-color: #ef4444;
            color: white;
        }
        .btn-danger:hover {
            background-color: #dc2626;
        }
        /* 输入框样式 */
        .input-text {
            border: 1px solid #d1d5db;
            padding: 0.625rem;
            border-radius: 0.375rem;
            width: 100%;
            margin-bottom: 1rem;
            box-shadow: inset 0 1px 2px 0 rgba(0,0,0,0.05);
        }
        .input-text:focus {
            border-color: #3b82f6;
            box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.3);
        }
        /* 状态显示样式 */
        .status {
            padding: 0.5rem;
            border-radius: 0.375rem;
            font-size: 0.875rem;
            word-wrap: break-word;
        }
        .status-info { background-color: #eff6ff; color: #3b82f6; }
        .status-success { background-color: #f0fdf4; color: #22c55e; }
        .status-error { background-color: #fef2f2; color: #ef4444; }
        .hidden { display: none; }
        h1 { font-size: 1.75rem; font-weight: 600; margin-bottom: 1rem; text-align: center; color: #111827; }
        label { display: block; margin-bottom: 0.25rem; font-weight: 500; color: #4b5563; }

        /* 聊天区域和消息气泡样式 */
        #chatArea {
            flex-grow: 1;
            overflow-y: auto;
            border: 1px solid #e5e7eb;
            padding: 1rem;
            border-radius: 0.5rem;
            background-color: #f9fafb;
            margin-bottom: 1rem;
            display: flex;
            flex-direction: column;
        }
        .message-container { display: flex; margin-bottom: 0.75rem; width: 100%; }
        .message-container.user { justify-content: flex-end; }
        .message-container.bot { justify-content: flex-start; }
        .message-container.system { justify-content: center; }
        .message-bubble { max-width: 85%; padding: 0.75rem 1.125rem; border-radius: 1.25rem; word-wrap: break-word; position: relative; font-size: 0.9rem; line-height: 1.5; }
        .message-bubble.user { background-color: #2563eb; color: white; border-bottom-right-radius: 0.375rem; }
        .message-bubble.bot { background-color: #e5e7eb; color: #1f2937; border-bottom-left-radius: 0.375rem; }
        /* Markdown 渲染后的基本样式 */
        .message-text p:last-child { margin-bottom: 0; }
        .message-text ul, .message-text ol { padding-left: 1.5em; margin: 0.5em 0; }
        .message-text pre { background-color: #d1d5db; padding: 0.75em; margin: 0.5em 0; border-radius: 0.25rem; overflow-x: auto; font-size: 0.85em;}
        .message-text code { font-family: 'Courier New', Courier, monospace; background-color: #d1d5db; padding: 0.1em 0.3em; border-radius: 0.2rem; }
        .message-text pre code { background-color: transparent; padding: 0; }
        .message-text blockquote { border-left: 3px solid #9ca3af; padding-left: 1em; margin-left: 0; color: #4b5563; }
        .message-text table { border-collapse: collapse; width: 100%; margin: 1em 0; }
        .message-text th, .message-text td { border: 1px solid #d1d5db; padding: 0.5em; }
        .message-text th { background-color: #f3f4f6; }
    </style>
</head>
<body>
    <div class="container">
        <!-- 頂部控件區域 -->
        <div class="flex-shrink-0">
            <h1>智能對話助手</h1>
            <div class="grid grid-cols-2 gap-4 mb-2">
                <div>
                    <label for="wsUrlInput">WebSocket URL:</label>
                    <input type="text" id="wsUrlInput" class="input-text !mb-0" value="ws://localhost:8765">
                </div>
                 <div>
                    <label for="clientSessionIdInput">會話ID (UUID):</label>
                    <div class="flex">
                        <input type="text" id="clientSessionIdInput" class="input-text !mb-0" readonly>
                        <button id="generateUuidButton" class="btn btn-secondary text-sm py-1 px-2 ml-2">新ID</button>
                    </div>
                </div>
            </div>
            <div class="grid grid-cols-2 gap-4 mb-2">
                 <button id="connectButton" class="btn btn-primary w-full">連接</button>
                 <button id="disconnectButton" class="btn btn-secondary w-full hidden">斷開連接</button>
            </div>
            <div id="statusArea" class="status status-info text-center">狀態：未連接</div>
        </div>

        <!-- 聊天顯示區域 -->
        <div id="chatArea"></div>

        <!-- 輸入控制區域 -->
        <div class="flex-shrink-0">
            <textarea id="textInput" class="input-text" rows="2" placeholder="請先連接伺服器..." title="文本輸入框"></textarea>
            <div class="flex items-center">
                <button id="sendTextButton" class="btn btn-primary mr-2 flex-grow" disabled>
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-send-fill mr-2" viewBox="0 0 16 16"><path d="M15.964.686a.5.5 0 0 0-.65-.65L.767 5.855H.766l-.452.18a.5.5 0 0 0-.082.887l.41.26.001.002 4.995 3.178 3.178 4.995.002.002.26.41a.5.5 0 0 0 .886-.083zm-1.833 1.89L6.637 10.07l-.215-.338a.5.5 0 0 0-.154-.154l-.338-.215 7.494-7.494 1.178-.471z"/></svg>
                    發送
                </button>
                <button id="startRecordButton" class="btn btn-primary" disabled title="开始录音">
                     <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-mic-fill" viewBox="0 0 16 16"><path d="M5 3a3 3 0 0 1 6 0v5a3 3 0 0 1-6 0z"/><path d="M3.5 6.5A.5.5 0 0 1 4 7v1a4 4 0 0 0 8 0V7a.5.5 0 0 1 1 0v1a5 5 0 0 1-4.5 4.975V15h3a.5.5 0 0 1 0 1h-7a.5.5 0 0 1 0-1h3v-2.025A5 5 0 0 1 3 8V7a.5.5 0 0 1 .5-.5"/></svg>
                </button>
                <button id="stopRecordButton" class="btn btn-danger hidden" title="停止录音">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-stop-fill" viewBox="0 0 16 16"><path d="M5 3.5h6A1.5 1.5 0 0 1 12.5 5v6a1.5 1.5 0 0 1-1.5 1.5H5A1.5 1.5 0 0 1 3.5 11V5A1.5 1.5 0 0 1 5 3.5"/></svg>
                </button>
            </div>
            <div id="audioPlaybackStatus" class="status status-info mt-2 text-center">播放器: 空閒</div>
        </div>
    </div>

    <script>
        // DOM Elements
        const wsUrlInput = document.getElementById('wsUrlInput');
        const connectButton = document.getElementById('connectButton');
        const disconnectButton = document.getElementById('disconnectButton');
        const clientSessionIdInput = document.getElementById('clientSessionIdInput');
        const generateUuidButton = document.getElementById('generateUuidButton');
        const textInput = document.getElementById('textInput');
        const sendTextButton = document.getElementById('sendTextButton');
        const startRecordButton = document.getElementById('startRecordButton');
        const stopRecordButton = document.getElementById('stopRecordButton');
        const statusArea = document.getElementById('statusArea');
        const chatArea = document.getElementById('chatArea');
        const audioPlaybackStatus = document.getElementById('audioPlaybackStatus');

        // Global State
        let websocket = null;
        let micAudioContext = null;
        let mediaStream = null;
        let audioWorkletNode = null;
        let serverPlaybackAudioContext = null;
        let mainAccumulatedPcmData = new Float32Array(0);
        let isServerAudioPlaying = false;
        let currentSourceNode = null;
        let nextChunkPlayTime = 0;
        let currentBotMessageBubble = null;
        let currentFullMarkdownText = '';
        const clientTagId = 'web_client_tag_' + Math.random().toString(36).substr(2, 5);

        // Constants
        const TARGET_AUDIO_SAMPLE_RATE = 16000;
        const EventType = {
            CLIENT_TEXT_INPUT: "CLIENT_TEXT_INPUT",
            SERVER_TEXT_RESPONSE: "SERVER_TEXT_RESPONSE",
            SERVER_AUDIO_RESPONSE: "SERVER_AUDIO_RESPONSE",
            SERVER_SYSTEM_MESSAGE: "SERVER_SYSTEM_MESSAGE",
            SYSTEM_CLIENT_SESSION_START: "SYSTEM_CLIENT_SESSION_START",
            SYSTEM_SERVER_SESSION_START: "SYSTEM_SERVER_SESSION_START",
        };

        // --- AudioWorklet Code (Simplified for direct binary streaming) ---
        const audioProcessorCode = `
            class AudioStreamerProcessor extends AudioWorkletProcessor {
                constructor(options) {
                    super(options);
                    this.inputSampleRate = options.processorOptions.inputSampleRate;
                    this.outputSampleRate = options.processorOptions.outputSampleRate;
                    this.resampleRatio = this.inputSampleRate / this.outputSampleRate;
                    this.port.onmessage = (event) => {}; // Keep port open
                }

                resample(inputSamples) {
                    if (this.resampleRatio === 1) return inputSamples;
                    const outputSamples = [];
                    for (let i = 0; i < inputSamples.length / this.resampleRatio; i++) {
                        const index = i * this.resampleRatio;
                        const floor = Math.floor(index);
                        const ceil = Math.ceil(index);
                        if (ceil < inputSamples.length) {
                            const value = inputSamples[floor] + (inputSamples[ceil] - inputSamples[floor]) * (index - floor);
                            outputSamples.push(value);
                        }
                    }
                    return new Float32Array(outputSamples);
                }

                process(inputs, outputs, parameters) {
                    const inputChannelData = inputs[0]?.[0];
                    if (!inputChannelData) return true;

                    const resampled = this.resample(inputChannelData);

                    if (resampled.length > 0) {
                         // Post the raw PCM data back to the main thread
                         this.port.postMessage(resampled.buffer, [resampled.buffer]);
                    }

                    return true;
                }
            }
            registerProcessor('audio-streamer-processor', AudioStreamerProcessor);
        `;

        // --- Core Functions ---

        function updateStatus(message, type = 'info') {
            statusArea.textContent = `狀態：${message}`;
            statusArea.className = `status status-${type} text-center`;
        }

        function generateAndSetUuid() {
            const newUuid = self.crypto.randomUUID();
            clientSessionIdInput.value = newUuid;
            localStorage.setItem('clientSessionIdForChatbot', newUuid);
            return newUuid;
        }

        marked.setOptions({ gfm: true, breaks: true, pedantic: false });

        function finalizePreviousBotMessage() {
            if (currentBotMessageBubble && currentFullMarkdownText) {
                const messageTextDiv = currentBotMessageBubble.querySelector('.message-text');
                if (messageTextDiv) {
                    messageTextDiv.innerHTML = marked.parse(currentFullMarkdownText);
                }
            }
            currentBotMessageBubble = null;
            currentFullMarkdownText = '';
        }

        function updateAudioPlaybackStatus(message) {
            audioPlaybackStatus.textContent = `播放器: ${message}`;
        }

        function appendMessageToDisplay(text, type, isFinal = true) {
            if (type !== 'bot') finalizePreviousBotMessage();

            if (type === 'bot') {
                if (!currentBotMessageBubble) {
                    const messageContainer = document.createElement('div');
                    messageContainer.classList.add('message-container', 'bot');
                    const bubble = document.createElement('div');
                    bubble.classList.add('message-bubble', 'bot');
                    const messageTextDiv = document.createElement('div');
                    messageTextDiv.classList.add('message-text');
                    bubble.appendChild(messageTextDiv);
                    messageContainer.appendChild(bubble);
                    chatArea.appendChild(messageContainer);
                    currentBotMessageBubble = bubble;
                    currentFullMarkdownText = '';
                }
                currentFullMarkdownText += text;
                const messageTextDiv = currentBotMessageBubble.querySelector('.message-text');
                if (messageTextDiv) messageTextDiv.innerHTML = marked.parse(currentFullMarkdownText);
                if (isFinal) {
                    currentBotMessageBubble = null;
                    currentFullMarkdownText = '';
                }
            } else {
                const messageContainer = document.createElement('div');
                messageContainer.classList.add('message-container', type);
                const bubble = document.createElement('div');
                bubble.classList.add('message-bubble', type);
                bubble.textContent = text;
                messageContainer.appendChild(bubble);
                chatArea.appendChild(messageContainer);
            }
            chatArea.scrollTop = chatArea.scrollHeight;
        }

        async function initializeServerPlaybackAudioContext() {
            if (serverPlaybackAudioContext && serverPlaybackAudioContext.state !== 'closed') return;
            try {
                serverPlaybackAudioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: TARGET_AUDIO_SAMPLE_RATE });
                if (serverPlaybackAudioContext.state === 'suspended') await serverPlaybackAudioContext.resume();
                updateAudioPlaybackStatus("已準備");
            } catch (e) {
                updateAudioPlaybackStatus(`錯誤: ${e.message}`);
            }
        }

        function playNextLargeAudioChunk() {
            if (isServerAudioPlaying || !serverPlaybackAudioContext) return;
            const accumulatedDuration = mainAccumulatedPcmData.length / TARGET_AUDIO_SAMPLE_RATE;
            if (accumulatedDuration < 0.1) return;
            isServerAudioPlaying = true;

            const samplesToPlay = mainAccumulatedPcmData.length;
            const pcmChunkToPlay = mainAccumulatedPcmData.slice(0, samplesToPlay);
            mainAccumulatedPcmData = mainAccumulatedPcmData.slice(samplesToPlay);

            const audioBufferToPlay = serverPlaybackAudioContext.createBuffer(1, pcmChunkToPlay.length, TARGET_AUDIO_SAMPLE_RATE);
            audioBufferToPlay.getChannelData(0).set(pcmChunkToPlay);

            currentSourceNode = serverPlaybackAudioContext.createBufferSource();
            currentSourceNode.buffer = audioBufferToPlay;
            currentSourceNode.connect(serverPlaybackAudioContext.destination);
            currentSourceNode.onended = () => {
                isServerAudioPlaying = false;
                currentSourceNode = null;
                if (mainAccumulatedPcmData.length > 0) playNextLargeAudioChunk();
                else updateAudioPlaybackStatus("空閒");
            };

            let scheduledPlayTime = nextChunkPlayTime;
            if (scheduledPlayTime < serverPlaybackAudioContext.currentTime) scheduledPlayTime = serverPlaybackAudioContext.currentTime;

            updateAudioPlaybackStatus(`正在播放...`);
            currentSourceNode.start(scheduledPlayTime);
            nextChunkPlayTime = scheduledPlayTime + audioBufferToPlay.duration;
        }

        function connectWebSocket() {
            const wsUrl = wsUrlInput.value;
            if (websocket && websocket.readyState === WebSocket.OPEN) return;

            updateStatus('正在連接...', 'info');
            websocket = new WebSocket(wsUrl);
            websocket.binaryType = "arraybuffer";

            websocket.onopen = async () => {
                updateStatus('連接成功', 'success');
                connectButton.classList.add('hidden');
                disconnectButton.classList.remove('hidden');
                sendTextButton.disabled = false;
                startRecordButton.disabled = false;
                textInput.placeholder = "請輸入消息...";
                await initializeServerPlaybackAudioContext();

                const initMessage = {
                    event_type: EventType.SYSTEM_CLIENT_SESSION_START,
                    tag_id: clientTagId,
                    event_data: { client_type: "web_client_binary_stream" },
                    session_id: clientSessionIdInput.value
                };
                websocket.send(JSON.stringify(initMessage));
                appendMessageToDisplay(`已發送會話初始化請求 (ID: ${clientSessionIdInput.value})`, 'system');
            };

            // *** UPDATED: Added comments to clarify dual-mode handling ***
            websocket.onmessage = async (event) => {
                // PATH 1: Handle JSON-based text, commands, and Base64 audio events
                if (typeof event.data === 'string') {
                    console.log("[WebSocket] Received String (JSON) message.");
                    try {
                        const jsonData = JSON.parse(event.data);
                        const eventType = jsonData.event_type;
                        const eventData = jsonData.event_data;

                        switch (eventType) {
                            case EventType.SYSTEM_SERVER_SESSION_START:
                                const newSessionId = jsonData.session_id;
                                clientSessionIdInput.value = newSessionId;
                                localStorage.setItem('clientSessionIdForChatbot', newSessionId);
                                appendMessageToDisplay(`伺服器已確認會話 (ID: ${newSessionId})`, 'system');
                                break;
                            case EventType.SERVER_TEXT_RESPONSE:
                                appendMessageToDisplay(eventData.text || '', 'bot', eventData.is_final);
                                break;
                            case EventType.SERVER_AUDIO_RESPONSE:
                                if (eventData && typeof eventData.data === 'string') {
                                    // This handles audio sent as a Base64 string within a JSON object
                                    console.log(`[WebSocket] Handling Base64 audio from JSON event...`);
                                    try {
                                        const binaryString = atob(eventData.data);
                                        const len = binaryString.length;
                                        const bytes = new Uint8Array(len);
                                        for (let i = 0; i < len; i++) bytes[i] = binaryString.charCodeAt(i);
                                        const audioArrayBuffer = bytes.buffer;

                                        // Feed the decoded binary data into the standard playback pipeline
                                        if (!serverPlaybackAudioContext) await initializeServerPlaybackAudioContext();
                                        const decodedAudioBuffer = await serverPlaybackAudioContext.decodeAudioData(audioArrayBuffer);
                                        const pcmData = decodedAudioBuffer.getChannelData(0);
                                        const newBuffer = new Float32Array(mainAccumulatedPcmData.length + pcmData.length);
                                        newBuffer.set(mainAccumulatedPcmData);
                                        newBuffer.set(pcmData, mainAccumulatedPcmData.length);
                                        mainAccumulatedPcmData = newBuffer;
                                        playNextLargeAudioChunk();
                                    } catch(e) { console.error("處理 Base64 音頻數據時出錯:", e); }
                                }
                                break;
                        }
                    } catch (e) { console.error("解析伺服器 JSON 消息失敗:", e, event.data); }

                // PATH 2: Handle raw binary audio messages directly
                } else if (event.data instanceof ArrayBuffer) {
                    console.log(`[WebSocket] Received raw ArrayBuffer (binary audio) message. Size: ${event.data.byteLength} bytes.`);
                    if (!serverPlaybackAudioContext) await initializeServerPlaybackAudioContext();
                    try {
                        // This assumes the ArrayBuffer is a complete audio file chunk (like a WAV)
                        // that decodeAudioData can understand.
                        const decodedAudioBuffer = await serverPlaybackAudioContext.decodeAudioData(event.data);
                        const pcmData = decodedAudioBuffer.getChannelData(0);

                        // Add the decoded PCM data to our playback buffer
                        const newBuffer = new Float32Array(mainAccumulatedPcmData.length + pcmData.length);
                        newBuffer.set(mainAccumulatedPcmData);
                        newBuffer.set(pcmData, mainAccumulatedPcmData.length);
                        mainAccumulatedPcmData = newBuffer;

                        // Trigger the playback logic
                        playNextLargeAudioChunk();
                    } catch(e) { console.error("直接二進位音頻解碼失敗:", e); }
                }
            };

            websocket.onclose = () => {
                updateStatus('已斷開連接', 'info');
                websocket = null;
                connectButton.classList.remove('hidden');
                disconnectButton.classList.add('hidden');
                sendTextButton.disabled = true;
                startRecordButton.disabled = true;
                stopRecording(false);
                finalizePreviousBotMessage();
            };
        }

        function disconnectWebSocket() { if (websocket) websocket.close(); }

        async function startRecording() {
            if (mediaStream) return;
            startRecordButton.classList.add('hidden');
            stopRecordButton.classList.remove('hidden');
            try {
                micAudioContext = new (window.AudioContext || window.webkitAudioContext)();
                mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                const source = micAudioContext.createMediaStreamSource(mediaStream);
                const audioWorkletUrl = URL.createObjectURL(new Blob([audioProcessorCode], { type: 'application/javascript' }));
                await micAudioContext.audioWorklet.addModule(audioWorkletUrl);
                audioWorkletNode = new AudioWorkletNode(micAudioContext, 'audio-streamer-processor', {
                    processorOptions: { inputSampleRate: micAudioContext.sampleRate, outputSampleRate: TARGET_AUDIO_SAMPLE_RATE }
                });

                audioWorkletNode.port.onmessage = (event) => {
                    // The event.data is now a raw ArrayBuffer (Float32Array's buffer)
                    const pcmFloat32Data = event.data;

                    // Convert Float32 PCM to Int16 PCM for transport
                    const pcmInt16 = new Int16Array(pcmFloat32Data.byteLength / 4);
                    const view = new DataView(pcmFloat32Data);
                    for (let i = 0; i < pcmInt16.length; i++) {
                        const float = view.getFloat32(i * 4, true); // Little-endian
                        pcmInt16[i] = Math.max(-32768, Math.min(32767, float * 32767));
                    }

                    if (websocket && websocket.readyState === WebSocket.OPEN) {
                        // Send the raw binary data (the buffer of the Int16Array)
                        websocket.send(pcmInt16.buffer);
                    }
                };
                source.connect(audioWorkletNode);
                appendMessageToDisplay('開始錄音...', 'system');
            } catch (err) {
                console.error("麥克風訪問失敗:", err);
                appendMessageToDisplay(`麥克風錯誤: ${err.message}`, 'system');
                stopRecording(false);
            }
        }

        function stopRecording(sendEndSignal = true) {
            if (mediaStream) mediaStream.getTracks().forEach(track => track.stop());
            if (micAudioContext) micAudioContext.close();
            mediaStream = null;
            micAudioContext = null;
            audioWorkletNode = null;
            startRecordButton.classList.remove('hidden');
            stopRecordButton.classList.add('hidden');
        }

        // Event Listeners
        connectButton.onclick = connectWebSocket;
        disconnectButton.onclick = disconnectWebSocket;
        generateUuidButton.onclick = generateAndSetUuid;
        startRecordButton.onclick = startRecording;
        stopRecordButton.onclick = stopRecording;

        sendTextButton.onclick = () => {
            const text = textInput.value.trim();
            if (text && websocket && websocket.readyState === WebSocket.OPEN) {
                appendMessageToDisplay(text, 'user');
                const message = {
                    event_type: EventType.CLIENT_TEXT_INPUT,
                    tag_id: clientTagId,
                    event_data: { text: text },
                    session_id: clientSessionIdInput.value
                };
                websocket.send(JSON.stringify(message));
                textInput.value = '';
            }
        };
        textInput.addEventListener('keypress', (e) => {
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                sendTextButton.click();
            }
        });

        // Initialize
        clientSessionIdInput.value = localStorage.getItem('clientSessionIdForChatbot') || generateAndSetUuid();
    </script>
</body>
</html>
